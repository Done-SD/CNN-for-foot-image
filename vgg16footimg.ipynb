{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of images 168\n"
     ]
    }
   ],
   "source": [
    "path = './dataset/'\n",
    "files = glob(os.path.join(path, '*/*/*.png'))\n",
    "\n",
    "print ( \"Total no of images\", len(files) )\n",
    "\n",
    "# 현재 데이터셋의 수를 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'In': 0, 'Out': 1}\n",
      "['In', 'Out']\n"
     ]
    }
   ],
   "source": [
    "simple_transform = transforms.Compose([transforms.Resize((60,60))\n",
    "                        , transforms.ToTensor()\n",
    "                        , transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train = ImageFolder('dataset/train/', simple_transform)\n",
    "valid = ImageFolder('dataset/valid/', simple_transform)\n",
    "\n",
    "print(train.class_to_idx)\n",
    "print(train.classes)\n",
    "\n",
    "# 라벨링 체크 및 데이터 로더 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = torch.utils.data.DataLoader(train,shuffle=True,batch_size=4,num_workers=3)\n",
    "valid_data_gen = torch.utils.data.DataLoader(valid,batch_size=4,num_workers=3)\n",
    "\n",
    "# 데이터 로더 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {'train':len(train_data_gen.dataset),'valid':len(valid_data_gen.dataset)}\n",
    "\n",
    "dataloaders = {'train':train_data_gen,'valid':valid_data_gen}\n",
    "\n",
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512, eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(4608, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv7): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv8): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv9): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv10): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear1): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "\n",
    "# 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/138 (0%)]\tLoss: 6.576874\n",
      "Train Epoch: 1 [80/138 (57%)]\tLoss: 0.859234\n",
      "[1] Train Loss: 1.1087, Accuracy: 52.17%\n",
      "[1] Test Loss: 0.9416, Accuracy: 50.00%\n",
      "Train Epoch: 2 [0/138 (0%)]\tLoss: 0.874874\n",
      "Train Epoch: 2 [80/138 (57%)]\tLoss: 1.416640\n",
      "[2] Train Loss: 1.5457, Accuracy: 70.29%\n",
      "[2] Test Loss: 1.4899, Accuracy: 70.00%\n",
      "Train Epoch: 3 [0/138 (0%)]\tLoss: 2.094484\n",
      "Train Epoch: 3 [80/138 (57%)]\tLoss: 0.508054\n",
      "[3] Train Loss: 0.1667, Accuracy: 93.48%\n",
      "[3] Test Loss: 0.8434, Accuracy: 80.00%\n",
      "Train Epoch: 4 [0/138 (0%)]\tLoss: 0.010135\n",
      "Train Epoch: 4 [80/138 (57%)]\tLoss: 0.097742\n",
      "[4] Train Loss: 0.8363, Accuracy: 81.16%\n",
      "[4] Test Loss: 0.6863, Accuracy: 80.00%\n",
      "Train Epoch: 5 [0/138 (0%)]\tLoss: 0.471290\n",
      "Train Epoch: 5 [80/138 (57%)]\tLoss: 0.730839\n",
      "[5] Train Loss: 0.0689, Accuracy: 97.10%\n",
      "[5] Test Loss: 0.6414, Accuracy: 80.00%\n",
      "Train Epoch: 6 [0/138 (0%)]\tLoss: 0.008617\n",
      "Train Epoch: 6 [80/138 (57%)]\tLoss: 0.064200\n",
      "[6] Train Loss: 0.0527, Accuracy: 97.83%\n",
      "[6] Test Loss: 0.6419, Accuracy: 80.00%\n",
      "Train Epoch: 7 [0/138 (0%)]\tLoss: 0.008904\n",
      "Train Epoch: 7 [80/138 (57%)]\tLoss: 0.002338\n",
      "[7] Train Loss: 0.1661, Accuracy: 94.20%\n",
      "[7] Test Loss: 1.3319, Accuracy: 76.67%\n",
      "Train Epoch: 8 [0/138 (0%)]\tLoss: 0.006557\n",
      "Train Epoch: 8 [80/138 (57%)]\tLoss: 0.000075\n",
      "[8] Train Loss: 0.0667, Accuracy: 97.10%\n",
      "[8] Test Loss: 1.4391, Accuracy: 70.00%\n",
      "Train Epoch: 9 [0/138 (0%)]\tLoss: 0.000641\n",
      "Train Epoch: 9 [80/138 (57%)]\tLoss: 0.000040\n",
      "[9] Train Loss: 0.0525, Accuracy: 97.10%\n",
      "[9] Test Loss: 0.9991, Accuracy: 80.00%\n",
      "Train Epoch: 10 [0/138 (0%)]\tLoss: 0.007975\n",
      "Train Epoch: 10 [80/138 (57%)]\tLoss: 0.447540\n",
      "[10] Train Loss: 0.0534, Accuracy: 97.83%\n",
      "[10] Test Loss: 1.0264, Accuracy: 80.00%\n",
      "Train Epoch: 11 [0/138 (0%)]\tLoss: 0.008356\n",
      "Train Epoch: 11 [80/138 (57%)]\tLoss: 0.609189\n",
      "[11] Train Loss: 0.0672, Accuracy: 96.38%\n",
      "[11] Test Loss: 0.9621, Accuracy: 80.00%\n",
      "Train Epoch: 12 [0/138 (0%)]\tLoss: 0.002264\n",
      "Train Epoch: 12 [80/138 (57%)]\tLoss: 0.000729\n",
      "[12] Train Loss: 0.2300, Accuracy: 87.68%\n",
      "[12] Test Loss: 0.7297, Accuracy: 76.67%\n",
      "Train Epoch: 13 [0/138 (0%)]\tLoss: 0.476825\n",
      "Train Epoch: 13 [80/138 (57%)]\tLoss: 0.039571\n",
      "[13] Train Loss: 0.1275, Accuracy: 95.65%\n",
      "[13] Test Loss: 2.0186, Accuracy: 70.00%\n",
      "Train Epoch: 14 [0/138 (0%)]\tLoss: 0.117391\n",
      "Train Epoch: 14 [80/138 (57%)]\tLoss: 0.008747\n",
      "[14] Train Loss: 0.1860, Accuracy: 87.68%\n",
      "[14] Test Loss: 1.0580, Accuracy: 70.00%\n",
      "Train Epoch: 15 [0/138 (0%)]\tLoss: 0.844307\n",
      "Train Epoch: 15 [80/138 (57%)]\tLoss: 0.505437\n",
      "[15] Train Loss: 0.0246, Accuracy: 98.55%\n",
      "[15] Test Loss: 1.7403, Accuracy: 73.33%\n",
      "Train Epoch: 16 [0/138 (0%)]\tLoss: 0.006936\n",
      "Train Epoch: 16 [80/138 (57%)]\tLoss: 0.002072\n",
      "[16] Train Loss: 0.0171, Accuracy: 99.28%\n",
      "[16] Test Loss: 1.4664, Accuracy: 76.67%\n",
      "Train Epoch: 17 [0/138 (0%)]\tLoss: 0.016357\n",
      "Train Epoch: 17 [80/138 (57%)]\tLoss: 0.001976\n",
      "[17] Train Loss: 0.0782, Accuracy: 95.65%\n",
      "[17] Test Loss: 1.9292, Accuracy: 70.00%\n",
      "Train Epoch: 18 [0/138 (0%)]\tLoss: 0.002959\n",
      "Train Epoch: 18 [80/138 (57%)]\tLoss: 0.047330\n",
      "[18] Train Loss: 0.0044, Accuracy: 100.00%\n",
      "[18] Test Loss: 1.8753, Accuracy: 76.67%\n",
      "Train Epoch: 19 [0/138 (0%)]\tLoss: 0.000337\n",
      "Train Epoch: 19 [80/138 (57%)]\tLoss: 0.003508\n",
      "[19] Train Loss: 0.0012, Accuracy: 100.00%\n",
      "[19] Test Loss: 2.1003, Accuracy: 76.67%\n",
      "Train Epoch: 20 [0/138 (0%)]\tLoss: 0.001817\n",
      "Train Epoch: 20 [80/138 (57%)]\tLoss: 0.000018\n",
      "[20] Train Loss: 0.0006, Accuracy: 100.00%\n",
      "[20] Test Loss: 2.2989, Accuracy: 73.33%\n",
      "\n",
      "\n",
      "학습이 완료되었습니다. \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_data_gen, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, valid_data_gen)\n",
    "    train_loss, train_accuracy = evaluate(model, train_data_gen)\n",
    "    \n",
    "    print('[{}] Train Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, train_loss, train_accuracy))\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))\n",
    "    \n",
    "print(\"\"\"\n",
    "\n",
    "학습이 완료되었습니다. \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
